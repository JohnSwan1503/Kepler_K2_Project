{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid black\"> </hr>\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Data Import\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# For Data Wrangling and Visualizing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# For Reading FITS Data From NASA\n",
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For Processing the huge amount of Light Curve Data\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import pandas.testing as pdt\n",
    "from __future__ import print_function\n",
    "# import swifter\n",
    "# import mapply\n",
    "\n",
    "import sys\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import partial\n",
    "from multiprocessing import Pool, RLock, freeze_support\n",
    "from random import random\n",
    "from threading import RLock as TRLock\n",
    "from tqdm.notebook import tqdm, trange, tqdm_notebook\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "# from progressbar import progressbar\n",
    "import time \n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:.3px solid grey\"> </hr>\n",
    "\n",
    "## Load in FITS Light Curve URLs\n",
    "Here we will load in and clean up the wget links for each FITS lightcurve file for each K2 campaign. Each link represents a time series of the flux values observed. The imported \"astropy.io\" can read these links and return the useful data for model access.\n",
    "\n",
    "NB: \n",
    "- Campaign 9 is notably missing, as that data has not been made publicly available through the Mikulski Archive. This unfortunately will reduce the number of \"confirmed/candidate\" target light curve data points to pick from.\n",
    "- Campaigns 10a and 10b & 11a and 11b will be merged into Campaign 10 and Campaign 11, respectively, as no distinction is made in the K2 Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe from All Light Curve Files\n",
    "Read in and concatenate all files in the \"wgets\" folder using glob to parametrically generate a list of file path names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_curves = pd.concat([pd.read_fwf(file, header=None, names=[0, 1, \"Link\"]) for file in glob.glob(os.path.join(\".\\Data_Dump\\wgets\\Light_Curves\\\\\" , \"*.txt\"))], ignore_index=True).drop([0,1], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a Campaign, EPIC ID, and Cadence Column for Merging and Filtering Later\n",
    "A lot of unique identifying information is extracted from each of the FITS link strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_curves[\"Campaign\"] = light_curves[\"Link\"].str.extract(\"(?:s/c)(\\d{1,2})\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_curves[\"EPIC\"] = light_curves[\"Link\"].str.extract(\"(?:ktwo)(\\d+)\").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_curves[\"Cadence\"] = light_curves[\"Link\"].str[-8:-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out Short Cadence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_curves = light_curves.loc[light_curves.Cadence == \"llc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the EPIC IDs and Campaign No.s as a multi-index\n",
    "While neither are unique identifiers on their own, the combination of the two IDs is unique and will be usefull for merging the links to the K2 Targets data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_curves = light_curves.set_index([\"EPIC\", \"Campaign\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:.3px solid grey\"> </hr>\n",
    "\n",
    "## Load in Kepler Survey Disposition Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k1 = pd.concat([pd.read_csv(file, header=0, names=[\"KepID\", \"2MASS\", \"Label\"], skiprows=1) \n",
    "                       for file in glob.glob(os.path.join(\".\\Data_Dump\\Kepler_Labels\\\\\", \"*.txt\"))], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the Label column into individual classes and Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k1[\"K1_False_Positive\"] = np.where(labels_k1[\"Label\"].str.contains(\"False_Positive\", case=False), 1, 0)\n",
    "labels_k1[\"K1_Confirmed\"] = np.where(labels_k1[\"Label\"].str.contains(\"Exoplanet\", case=False), 1, 0)\n",
    "labels_k1[\"K1_Candidate\"] = np.where(labels_k1[\"Label\"].str.contains(\"Planetary_candidate\", case=False), 1, 0)\n",
    "labels_k1[\"K1_Binary\"] = np.where(labels_k1[\"Label\"].str.contains(\"Eclipsing_binary\", case=False), 1, 0)\n",
    "labels_k1[\"K1_Giant\"] = np.where(labels_k1[\"Label\"].str.contains(\"Red_giant\", case=False), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k1 = labels_k1.drop([\"Label\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:.3px solid grey\"> </hr>\n",
    "\n",
    "## Load in K2 Survey Disposition Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_candidate_data = ['loc_rowid', 'EPIC', '2MASS', 'EPIC.Name', 'pl_name',\n",
    "       'k2c_refdisp', 'k2c_reflink', 'Label', 'Campaign']\n",
    "columns_candidate_data = ['EPIC', 'Label', 'Campaign', 'EPIC.Name']\n",
    "labels_k2 = pd.read_csv(\"./Data_Dump/K2-Candidates.txt\", header=0, names=names_candidate_data, \n",
    "                        usecols=columns_candidate_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k2[\"K2_False_Positive\"] = np.where(labels_k2[\"Label\"].str.contains(\"False Positive\", case=False), 1, 0)\n",
    "labels_k2[\"K2_Confirmed\"] = np.where(labels_k2[\"Label\"].str.contains(\"confirmed\", case=False), 1, 0)\n",
    "labels_k2[\"K2_Candidate\"] = np.where(labels_k2[\"Label\"].str.contains(\"candidate\", case=False), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k2 = labels_k2.drop([\"Label\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k2 = labels_k2[labels_k2[\"Campaign\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k2[\"EPIC\"] = labels_k2[\"EPIC\"].str.extract(\"(\\d+)\").astype(int)\n",
    "labels_k2[\"Campaign\"] = labels_k2[\"Campaign\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k2[\"Event\"] = labels_k2[\"EPIC.Name\"].str[-2:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "candidates = pd.DataFrame({'EPIC': [x for x in labels_k2.EPIC.unique()], \n",
    "                           'count': [np.sort(labels_k2.loc[labels_k2.EPIC==x].Event.unique())[-1] for x in labels_k2.EPIC.unique()]}).set_index(['EPIC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(candidates)\n",
    "print(candidates.value_counts(normalize=True)*100)\n",
    "labels_k2.value_counts(subset=['K2_False_Positive', 'K2_Confirmed', 'K2_Candidate'], normalize=True)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k2 = labels_k2.set_index(['EPIC', 'Campaign'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_k2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EPIC.Name</th>\n",
       "      <th>K2_False_Positive</th>\n",
       "      <th>K2_Confirmed</th>\n",
       "      <th>K2_Candidate</th>\n",
       "      <th>Event</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Campaign</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EPIC 206358352.01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  EPIC.Name  K2_False_Positive  K2_Confirmed  K2_Candidate  \\\n",
       "Campaign                                                                     \n",
       "3         EPIC 206358352.01                  0             0             1   \n",
       "\n",
       "          Event  \n",
       "Campaign         \n",
       "3             1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_k2.xs(206358352)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'index' : (x for x in merge_labels_k2.index.unique)})\n",
    "# merge_k2 = pd.DataFrame({'index' : (x for x in labels_k2.index.unique())})\n",
    "# merge_k2[['EPIC', 'Campaign']] = pd.DataFrame(merge_k2['index'].tolist(), index=merge_k2.index)\n",
    "# merge_k2.drop(['index'], axis=1, inplace=True)\n",
    "# merge_k2.set_index(['EPIC', 'Campaign'], inplace=True)\n",
    "# merge_k2[['Confirmed', 'Candidate', 'False_Positive']] = labels_k2[x] for x in merge_k2.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:.3px solid grey\"> </hr>\n",
    "\n",
    "## Load in Target Data\n",
    "Here we will load in the ~400,000 targets described in the K2 Target Dataset and filter out non-stellar targets. Each row describes an object in the Ecliptic Plane Input Catalogue *(EPIC)* flagged for Transit Analysis by the Guest Observer Program. Targets observed across multiple campaigns will appear as multiple rows. For the purposes of this project the calibration and test field targets (Campaigns E and 0 respectively) will be dropped as these are by non-stellar targets such as asteroids, local planets, galaxies, or photometric artifacts to be calibrated out of future observation campaigns. Additionally, we will be removing targets with Object Type \"Extended\" and \"Null\", as these represent non-stellar observations part of the extended mission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit: Targets from Campaign 11 will also be dropped due to the ~14000 duplicate light curve observations and lack of any flagged transit events. Reviewing the press on the particular campaign, the observations were focused on the core of the Milky Way galaxy and not orbital transit candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe of every EPIC Target across every Campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (0,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "names_target_data = [\"EPIC\", \"2MASS\", \"Campaign\", \"Obj_Type\", \"rastr\",\n",
    "                     \"decstr\",\"k2_propid\",\"Distance\",\"k2_disterr1\",\"k2_disterr2\",\n",
    "                     \"k2_teff\",\"k2_tefferr1\",\"k2_tefferr2\",\"Stellar_Radius\",\"k2_raderr1\",\n",
    "                     \"k2_raderr2\",\"Stellar_Mass\",\"k2_masserr1\",\"k2_masserr2\",\"k2_kepmag\",\n",
    "                     \"k2_kepmagerr\",\"k2_kepmagflag\",\"k2_vjmag\",\"k2_vjmagerr\",\"k2_kmag\",\"k2_kmagerr\"]\n",
    "columns_target_data = [\"EPIC\", \"2MASS\", \"Campaign\", \"Obj_Type\"]\n",
    "target_data = pd.read_csv(\"./Data_Dump/K2-Targets.txt\", header=0,names=names_target_data, usecols=columns_target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Unwanted Targets and Fixing DTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data=target_data[~target_data[\"Campaign\"].isin([\"E\", \"11\", 11])]\n",
    "target_data[\"Campaign\"] = target_data[\"Campaign\"].astype(int)\n",
    "target_data=target_data[~target_data[\"EPIC\"].isin([\";\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data[\"EPIC\"]=target_data[\"EPIC\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set multi-index to unique EPIC ID and Campaign Descriptor (similar to Light Curve Dataframe index structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_data=target_data.set_index([\"EPIC\", \"Campaign\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Light Curve FITS links to each Target\n",
    "**Note:** Not all K2 EPIC targets ended up having usable observation data. These rows will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_light_curves = target_data.merge(light_curves, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(target_light_curves.isnull().sum()/target_light_curves.shape[0]*100).to_frame(name='Percent Null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_light_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flux_data (link):\n",
    "    global pbar\n",
    "    pbar.update(1)\n",
    "    with fits.open(link, mode=\"readonly\", cache=False, lazy_load_hdus=True) as hdulist:\n",
    "        pdcsap_fluxes = list(hdulist[1].data['PDCSAP_FLUX'])\n",
    "        return pdcsap_fluxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_light_curves_dict = {key : target_light_curves.xs(key, level=1) for key in set(target_light_curves.index.get_level_values('Campaign'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23055c64b0824e65b76120af814405c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for x in list(target_light_curves_dict.keys()):\n",
    "    with tqdm(total=len(target_light_curves_dict[x]['Link'])) as pbar:\n",
    "        target_light_curves_dict[x]['Flux']=target_light_curves_dict[x].apply(lambda row: get_flux_data(row.Link), axis=1)\n",
    "#         target_light_curves_dict[x]['Flux']=np.vectorize(get_flux_data)(target_light_curves_dict[x]['Link'])\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vectorize(get_flux_data)(target_light_curves_dict[1].loc[201150515, 'Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import psutil\n",
    "# gives a single float value\n",
    "psutil.cpu_percent()\n",
    "# gives an object with many fields\n",
    "psutil.virtual_memory()\n",
    "# you can convert that object to a dictionary \n",
    "dict(psutil.virtual_memory()._asdict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(len(target_light_curves_dict[y].iloc[x].Flux_Len) for x in range(target_light_curves_dict[0].shape[0])) for y in range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(next((i for i, z in enumerate(target_light_curves_dict[1].iloc[x].Flux_Len[::-1]) if z != 0), None) for x in range(target_light_curves_dict[1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.count_nonzero(target_light_curves_dict[0].iloc[x].Flux_Len)+next((i for i, z in enumerate(target_light_curves_dict[0].iloc[x].Flux_Len) if z != 0), None)-len(target_light_curves_dict[0].iloc[x].Flux_Len) for x in range(target_light_curves_dict[0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(np.count_nonzero(target_light_curves_dict[1].iloc[x].Flux_Len)-len(target_light_curves_dict[1].iloc[x].Flux_Len) for x in range(target_light_curves_dict[1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize(data, func, num_of_processes=os.cpu_count()):\n",
    "    data_split = np.array_split(data, num_of_processes)\n",
    "    pool = Pool(num_of_processes)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "def run_on_subset(func, data_subset):\n",
    "    return data_subset.apply(func)\n",
    "\n",
    "def parallelize_on_rows(data, func, num_of_processes=8):\n",
    "    return parallelize(data, partial(run_on_subset, func), num_of_processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.cpu_count()-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in tqdm_notebook(list(target_light_curves_dict.keys()), desc=\"Overall\"):\n",
    "    target_light_curves_dict[x]['Flux'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_light_curves_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in tqdm_notebook(target_light_curves_dict.items(), desc=\"Overall\"):\n",
    "    print(type(df[1]))\n",
    "    df[1].Flux = parallelize_on_rows(df[1].Link, get_flux_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in target_light_curves_dict.items():\n",
    "    print(type(df[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_light_curves_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallelize_on_rows(target_light_curves['Link'], get_flux_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_dataframe(df, func):\n",
    "    num_processes = mp.cpu_count()-2\n",
    "    df_split = np.array_split(df, num_processes)\n",
    "    with mp.Pool(num_processes) as p:\n",
    "        df = pd.concat(p.map(func, df_split))\n",
    "    return df\n",
    "\n",
    "def parallelize_function(df):\n",
    "    df['Flux'] = df['Link'].apply(get_flux_data, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_light_curves = tqdm(parallelize_dataframe(target_light_curves, parallelize_function), total=target_light_curves.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_light_curves['Flux'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    def get_flux_data (row):\n",
    "        link = row.Link\n",
    "        with fits.open(link, mode=\"readonly\") as hdulist:\n",
    "            pdcsap_fluxes = hdulist[1].data['PDCSAP_FLUX']\n",
    "            return {row.index : pdcsap_fluxes}\n",
    "    r = process_map(get_flux_data, target_light_curves, max_workers=10, chunksize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(target_light_curves.index.get_level_values(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Edit -> nbextensions config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
